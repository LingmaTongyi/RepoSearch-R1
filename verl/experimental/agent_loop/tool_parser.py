# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import asyncio
import json
import logging
import os
from abc import ABC, abstractmethod
import time
import regex as re
from pydantic import BaseModel
from verl.request_model.main import request_bailian_internal_models
from verl.utils.rollout_trace import rollout_trace_op

logger = logging.getLogger(__file__)
logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))


class FunctionCall(BaseModel):
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class ToolParser(ABC):
    _registry: dict[str, type["ToolParser"]] = {}

    def __init__(self, tokenizer) -> None:
        self.tokenizer = tokenizer

    @abstractmethod
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        """Extract tool calls from the responses.

        Args:
            responses_ids (List[int]): The ids of the responses.

        Returns:
            Tuple[str, List[FunctionCall]]: Content and extracted tool calls.
        """
        raise NotImplementedError

    @classmethod
    def get_tool_parser(cls, name: str, tokenizer):
        if name not in cls._registry:
            raise ValueError(f"Unknown tool parser: {name}")
        return cls._registry[name](tokenizer)

    @classmethod
    def register(cls, name: str):
        def decorator(subclass: type[ToolParser]) -> type[ToolParser]:
            cls._registry[name] = subclass
            return subclass

        return decorator


# @ToolParser.register("hermes")
# class HermesToolParser(ToolParser):
#     """Adapted from https://github.com/vllm-project/vllm/blob/v0.9.1/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py"""

#     def __init__(self, tokenizer) -> None:
#         super().__init__(tokenizer)

#         self.tool_call_start_token: str = "<tool_call>"
#         self.tool_call_end_token: str = "</tool_call>"
#         self.tool_call_regex = re.compile(r"<tool_call>(.*?)</tool_call>", re.DOTALL)
#         self.backend_tool_call_regex = re.compile(r"<tool_call>(.*)", re.DOTALL)
#     @rollout_trace_op
#     async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
#         loop = asyncio.get_running_loop()
#         text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)
#         if self.tool_call_start_token not in text or self.tool_call_end_token not in text:
#             return text, []
#         # breakpoint()
#         matches = self.tool_call_regex.findall(text)
#         if len(matches) == 0:
#             matches = self.backend_tool_call_regex.findall(text)
#             logger.info(f"parse text to get Tool calls by backend_tool_call_regex: {matches}")
#         function_calls = []
#         for match in matches:
            
#             try:
#                 function_call = json.loads(match)
#                 name, arguments = function_call["name"], function_call["arguments"]
#                 function_calls.append(FunctionCall(name=name, arguments=json.dumps(arguments, ensure_ascii=False)))
#             except Exception as e:
#                 logger.info(f"Failed to decode tool call: {match}")
#                 logger.error(f"Failed to decode tool call: {e}")

#         # remaing text exclude tool call tokens
#         content = self.tool_call_regex.sub("", text)
#         # logger.info(f"Tool calls: {function_calls}")

#         return content, function_calls

# @ToolParser.register("hermes")
# class HermesToolParser(ToolParser):
#     """Adapted from https://github.com/vllm-project/vllm/blob/v0.9.1/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py"""

#     def __init__(self, tokenizer) -> None:
#         super().__init__(tokenizer)

#         self.tool_call_start_token: str = "<tool_call>"
#         self.tool_call_end_token: str = "</tool_call>"
#         self.tool_call_regex = re.compile(r"<tool_call>(.*?)</tool_call>", re.DOTALL)
#         self.backend_tool_call_regex = re.compile(r"<tool_call>(.*)", re.DOTALL)

#     @rollout_trace_op
    # async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
    #     loop = asyncio.get_running_loop()
    #     text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)
    #     if self.tool_call_start_token not in text or self.tool_call_end_token not in text:
    #         return text, []
    #     # breakpoint()
    #     matches = self.tool_call_regex.findall(text)
    #     if len(matches) == 0:
    #         matches = self.backend_tool_call_regex.findall(text)
    #         logger.info(f"parse text to get Tool calls by backend_tool_call_regex: {matches}")
    #     function_calls = []
    #     for match in matches:
            
    #         try:
    #             function_call = json.loads(match)
    #             name, arguments = function_call["name"], function_call["arguments"]
    #             function_calls.append(FunctionCall(name=name, arguments=json.dumps(arguments, ensure_ascii=False)))
    #         except Exception as e:
    #             logger.info(f"Failed to decode tool call: {match}")
    #             logger.error(f"Failed to decode tool call: {e}")

    #     # remaing text exclude tool call tokens
    #     content = self.tool_call_regex.sub("", text)
    #     # logger.info(f"Tool calls: {function_calls}")

    #     return content, function_calls

@ToolParser.register("hermes")
class BashToolParser(ToolParser):
    def __init__(self, tokenizer) -> None:
        super().__init__(tokenizer)

    @rollout_trace_op
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        loop = asyncio.get_running_loop()
        text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)

        if text.count('### Answer') > 0:
            return 'finish', []
        
        if not self.check_thought_action_format(text):
            return 'Each answer should only contain one pair of ### Thought and ### Action. Please check your response.', []

        commands = self.extract_commands(text.split('### Action:')[-1])
        if len(commands) == 0:
            return 'Your response does not follow the correct format for calling the tool, or you did not call the tool. Please check your response and try again.', []
        elif len(commands) > 1:
            return 'You can only call one tool at a time. Please check your response and try again.', []
        else:
            return 'success', commands


    def check_thought_action_format(self,text):
        thought_pattern = r"###\s*Thought:\s*(.*)"
        action_pattern = r"###\s*Action:\s*(.*)"
        thought_matches = re.findall(thought_pattern, text)
        action_matches = re.findall(action_pattern, text)
        return len(thought_matches) == 1 and len(action_matches) == 1
    def extract_commands(self,text):
        pattern = r'```bash([\s\S]*?)```'
        matches = re.findall(pattern, text)
        # logger.info(f"[extract_commands]Extracted from: {text}")
        commands = []
        try:
            for match in matches:
                filtered_lines = list(filter(None, match.split('\n')))
                if len(filtered_lines) > 0:
                    for line in filtered_lines:  
                        commands.append(line)
        except Exception as e:
            print(e)
            print(matches)
        # logger.info(f"[extract_commands]Extracted commands: {commands}")
        return commands
    async def extract_answer_and_calculate_reward(self, respone_txt: str, question, question_related_code, golden_answer):
        answer = self.extract_result_answer(respone_txt)
        reward = await self._calculate_llm_judge_reward(
            candidate_answer=answer,
            question=question,
            question_related_code=question_related_code,
            golden_answer=golden_answer
        )
        return answer, reward
    def extract_result_answer(self, text):
        # 定义正则表达式来匹配 `### Answer: answer`
        pattern = r"###\s*Answer:\s*(.*)"
        # 使用正则表达式查找匹配
        match = re.search(pattern, text, re.DOTALL)
        if match:
            # 如果找到匹配，返回answer这个字符串
            return match.group(1).strip()
        else:
            return None
    async def _calculate_llm_judge_reward(
        self, 
        candidate_answer: str, 
        question: str, 
        question_related_code: str, 
        golden_answer: str
    ) -> float:
        """Calculate reward using LLM judge."""
        
        # LLM judge prompt template
        TRAIN_LLM_JUDGE_SCORE = """
You are an impartial judge tasked with critically evaluating the quality of AI assistant responses to user questions.
You will be provided with:
1. A user question (possibly including code)
2. A reference answer
3. The AI assistant's answer

Begin your evaluation by thoroughly understanding the user question and reference answer,
and then rigorously assess the AI assistant's answer based on the following dimension: Completeness.

**IMPORTANT NOTE**: 
1. The reference answer may represent just one of many valid solutions. Evaluate based on factual correctness and effectiveness, even if the approach differs.
2. For questions involving code, pay special attention to both the explanation and code implementation.

After providing your detailed explanations, you must rate the answer on a strict scale of 1 to 100 in each of the following dimension.
You must focus solely on the specific criteria for each dimension when assigning a score:

## Score Dimensions

1. **Completeness**: Does the AI assistant's answer cover all aspects of the user question, or does it miss any critical points?
    - **Guidelines**:
        - Read the reference answer to identify all the key components that should be included in the response.
        - Carefully compare the AI assistant's answer with the reference answer to identify any missing critical points.
        - Ensure there are no crucial points omitted or overly brief explanations.
        - Consider whether the AI assistant's answer, even if using a different approach, addresses all aspects of the user's question.
    - **Scoring**:
        - **1-20**: Largely incomplete, many critical points missed; missing the majority of key aspects.
        - **21-40**: Significant omissions, partially complete; missing more than half of the key points.
        - **41-60**: Some omissions, but covers most key points; missing a few critical points.
        - **61-80**: Minor omissions, but mostly complete; missing one or two minor points.
        - **81-100**: Fully comprehensive, no points missed; covers all key aspects thoroughly.
    
You must adhere strictly to the response format to give the final verdict after evaluating the AI assistant's answer.
Response format is: 
## Judge's Evaluation
### **Completeness**:
[Your reasoning]
Final verdict is: [[Completeness: ?]].

## Input:
### User Question
{question}
{question_related_code}
### Reference Answer
{golden_answer}
### AI Assistant's Answer
{candidate_answer}
### Judge's Evaluation
"""
        
        try:
            # Format the prompt
            prompt = TRAIN_LLM_JUDGE_SCORE.format(
                question=question,
                question_related_code=question_related_code,
                golden_answer=golden_answer,
                candidate_answer=candidate_answer
            )
            response = None
            while True:
                try:
                    response = request_bailian_internal_models(
                        model_key='qwen3-coder-plus',
                        prompt=prompt,
                        messages=None,
                        tools=None
                    )
                    if response:
                        break
                    response = request_bailian_internal_models(
                        model_key='qwen3-coder-480b-a35b-instruct',
                        prompt=prompt,
                        messages=None,
                        tools=None
                    )
                    if response:
                        break
                    logger.warning(f"Invalid response structure, retry again, response is {response}")
                except Exception as e:
                    logger.warning(f"API request failed on : {e}, response is {response}")
                time.sleep(2)
            
            # Parse the score from response
            reward = self._parse_judge_score(response['choices'][0]['message']['content'])
            return reward
            
        except Exception as e:
            logger.error(f"Error in LLM judge calculation: {e}")
            return 0.0
    
    def _parse_judge_score(self, response: str) -> float:
        """Parse the completeness score from LLM judge response."""
        try:
            import re
            
            # Look for pattern like "[[Completeness: 85]]"
            pattern = r"\[\[Completeness:\s*(\d+)\]\]"
            match = re.search(pattern, response)
            if match:
                score = int(match.group(1))
                if score < 1:
                    reward = 0.0
                elif score < 21:
                    reward = 0.2
                elif score < 41:
                    reward = 0.4
                elif score < 61:
                    reward = 0.6
                elif score < 81:
                    reward = 0.8
                elif score < 101:
                    reward = 1.0
                else:
                    logger.info(f"[parse_judge_score] score error: {score}")
                return reward
            else:
                logger.warning(f"Could not parse judge score from response: {response}")
                return 0.0
                
        except Exception as e:
            logger.error(f"Error parsing judge score: {e}")
            return 0.0
        